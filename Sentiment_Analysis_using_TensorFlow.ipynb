{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_set_deepLearning\",\"rb\") as f:\n",
    "    feature_set = pickle.load(f)\n",
    "    \n",
    "random.shuffle( feature_set )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2259\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "batch_size = 100\n",
    "n_classes = 2\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 2259])\n",
    "y = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = []\n",
    "y_1 = []\n",
    "\n",
    "for feature in feature_set:\n",
    "    X_1.append(feature[0])\n",
    "    y_1.append(feature[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = .2\n",
    "\n",
    "train_X = X_1[:-int(len(X_1)*test_size)]\n",
    "test_X = X_1[-int(len(X_1)*test_size):]\n",
    "train_y = y_1[:-int(len(y_1)*test_size)]\n",
    "test_y = y_1[-int(len(y_1)*test_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model( data ):\n",
    "    hidden_layer_1 = { \"weights\": tf.Variable(tf.random_normal([2259,n_nodes_hl1])),\n",
    "                       \"biases\" : tf.Variable(tf.random_normal([n_nodes_hl1]))   }\n",
    "    \n",
    "    hidden_layer_2 = { \"weights\": tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),\n",
    "                       \"biases\" : tf.Variable(tf.random_normal([n_nodes_hl2]))   }\n",
    "    \n",
    "    hidden_layer_3 = { \"weights\": tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),\n",
    "                       \"biases\" : tf.Variable(tf.random_normal([n_nodes_hl3]))   }\n",
    "    \n",
    "    output_layer = { \"weights\" : tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),\n",
    "                     \"biases\"  : tf.Variable(tf.random_normal([n_classes]))  }\n",
    "    \n",
    "    hl1 = tf.matmul( data,hidden_layer_1[\"weights\"] ) + hidden_layer_1[\"biases\"]\n",
    "    hl1 = tf.tanh(hl1)\n",
    "    \n",
    "    hl2 = tf.matmul( hl1,hidden_layer_2[\"weights\"] ) + hidden_layer_2[\"biases\"]\n",
    "    hl2 = tf.tanh(hl2)\n",
    "    \n",
    "    hl3 = tf.matmul( hl2,hidden_layer_3[\"weights\"] ) + hidden_layer_3[\"biases\"]\n",
    "    hl3 = tf.tanh(hl3)\n",
    "    \n",
    "    output = tf.matmul( hl3,output_layer[\"weights\"] ) + output_layer[\"biases\"]\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch number  1  the cost is  [197.58635  404.88284   61.52147  179.78859  247.08765  192.10802\n",
      " 146.15952  200.5713   158.2302   178.45749  277.59088  135.07452\n",
      " 234.51987  218.39474  132.65988  273.29132   67.83119  229.96416\n",
      " 255.44382  304.58884  231.68338  215.19081  130.83563   83.65696\n",
      " 187.07373  289.05502  187.38448  119.76293  300.05768  176.96234\n",
      " 174.08539  233.64752  212.10468  205.15613  188.0693   203.83423\n",
      " 202.20367  202.09052  196.65044   72.489235 199.7216   224.56308\n",
      " 181.99721  102.11233  218.35341  182.32085  117.96892  149.32675\n",
      " 362.1993   150.35161  138.30109  159.75662  222.94849  135.27339\n",
      " 399.33386  143.50691  171.57082  263.86298  127.67159  129.62704\n",
      " 129.78905  124.949066 266.7333   145.04553   81.44773  231.22092\n",
      " 151.18042  204.43066  103.24902  120.09384  196.54362  264.6756\n",
      " 184.58176  161.88199  113.05541  181.2554   221.64214  176.0941\n",
      " 164.49162  127.2061   140.27312  175.17397  142.59143  233.40515\n",
      " 255.96164  164.05966  164.71613  342.9739   265.27625  190.5871\n",
      " 160.71594   67.62892  187.3489   167.17615  219.61906  228.66605\n",
      " 171.4225   234.70668  261.61285   85.900536]\n",
      "After epoch number  2  the cost is  [125.26956  249.9219    54.252678  53.55671   73.77692   94.92562\n",
      " 128.93202   73.65756  160.07474   74.83283  140.64514  123.59662\n",
      " 100.63429  124.99706   33.575413  95.364426  32.432037  88.13905\n",
      " 195.19894   68.03651  134.05067  158.24037   91.5243    19.096207\n",
      " 154.68799  185.70499   79.45696  103.88776  162.13704   29.213469\n",
      "  97.86269   64.39112   82.59853  102.484314  92.59405  130.93573\n",
      "  88.917564 109.302284  87.44244   37.966946 127.8738   127.47751\n",
      "  87.58264   53.468647  62.554634  59.51073   65.597916  82.29831\n",
      " 157.52344   66.11546   91.20316   70.74938  106.14795   99.18497\n",
      " 171.27744   26.60221   62.96696  126.7182    59.17979   82.13504\n",
      "  70.05808   99.93849  152.61696   87.8649    46.199814 114.733246\n",
      "  90.20681   48.928886  77.915146  65.22388   65.38541   96.26912\n",
      " 150.72276   69.084625  83.74322   41.668617  79.28599  109.161446\n",
      "  50.433273 118.19055   66.866776  51.28795   47.43297   98.771454\n",
      "  82.327126  93.27057   40.602688 170.70563  105.72542   76.5865\n",
      "  91.899666  32.393936  34.3178    95.32198   91.27544  129.47304\n",
      " 117.41169   97.33955  114.32477   68.44841 ]\n",
      "After epoch number  3  the cost is  [5.5809875e+01 1.7131331e+02 1.6621645e+01 4.0699440e+01 5.2292561e+01\n",
      " 5.9884506e+01 5.6405750e+01 5.7002182e+01 5.7694485e+01 2.4762129e+01\n",
      " 8.2690804e+01 5.0052822e+01 3.3158581e+01 7.6170044e+01 1.2551965e+01\n",
      " 1.9534739e+01 2.2835773e+01 5.5152412e+01 1.2097384e+02 2.9930929e+01\n",
      " 7.1223923e+01 5.8146603e+01 7.0855156e+01 1.9177013e+01 6.5569908e+01\n",
      " 1.2174329e+02 4.0969360e+01 5.9360741e+01 7.2314148e+01 9.6676529e-05\n",
      " 5.2736874e+01 4.3357533e+01 5.6588982e+01 5.4845974e+01 6.4416603e+01\n",
      " 5.8030415e+01 3.9507641e+01 6.4054047e+01 7.5577690e+01 1.2193559e+01\n",
      " 7.4745789e+01 1.9740667e+01 7.5039017e+01 1.2833702e+01 2.2340994e+01\n",
      " 5.6904202e+01 4.7039879e+01 7.8920035e+00 1.0629845e+02 9.7056465e+00\n",
      " 1.5147874e+01 1.3977990e+01 5.5239117e+01 3.7657547e+01 7.6613350e+01\n",
      " 1.0384226e+01 3.0641529e+01 2.9450932e+01 3.7264977e+01 4.1081448e+01\n",
      " 3.7695229e+01 2.4841373e+01 1.2183514e+02 4.8460476e+01 3.2014294e+01\n",
      " 2.0844118e+01 2.9593157e+01 3.6421104e+01 3.6030064e+01 1.5654792e+01\n",
      " 4.5563801e+01 6.7121826e+01 4.3766464e+01 4.1806583e+01 1.7372135e+01\n",
      " 4.3129171e-03 2.2367168e+01 1.7260490e+01 2.5513432e+00 5.2613159e+01\n",
      " 4.6925079e+01 5.3084122e+01 2.9514942e+01 4.5382866e+01 5.5054558e+01\n",
      " 7.3964317e+01 4.0374210e+01 1.1503320e+02 4.9529964e+01 4.3760578e+01\n",
      " 3.3260372e+01 1.8893190e+01 7.8048935e+00 4.5877266e+01 8.0514023e+01\n",
      " 1.4721288e+02 7.4807465e+01 4.8849583e+01 4.4585484e+01 5.1722107e+01]\n",
      "After epoch number  4  the cost is  [5.5705729e+00 1.0497558e+02 1.3624390e+00 1.9344454e+01 1.3881595e-02\n",
      " 4.2285587e+01 3.8626820e+01 3.3064690e+01 5.2262032e+01 1.0283411e+01\n",
      " 1.9121295e+00 1.6895409e+01 7.1803298e+00 5.6485271e+01 4.1092925e-02\n",
      " 1.1714640e+01 1.6919996e-01 2.9092939e+01 9.1231110e+01 1.1259404e+01\n",
      " 1.8757229e+01 3.1051233e+01 3.4805420e+01 3.9290515e-01 4.3414909e+01\n",
      " 7.1435738e+01 5.3780910e+01 3.4970631e+01 3.4457138e+01 3.1679917e-02\n",
      " 2.1959135e+01 1.8348097e+01 3.7492619e+01 1.6141890e+01 1.3932018e+01\n",
      " 3.9975822e+01 1.3053908e+01 3.4539043e+01 1.7937855e+01 3.3298090e-02\n",
      " 2.0583338e+01 1.5451542e-01 2.6172375e+01 7.3714301e-02 2.1485274e-03\n",
      " 2.0452026e+01 2.5004160e+01 1.0875166e+01 8.5400604e+01 2.3794220e-01\n",
      " 3.0884211e+01 4.6763024e+00 4.5922535e+01 1.2597032e+01 5.3303726e+01\n",
      " 8.3346903e-01 1.9245903e+01 1.3579559e+01 1.5149595e+01 8.6763830e+00\n",
      " 3.9225395e+00 1.7384802e+01 8.3850754e+01 3.6545883e+01 1.9768356e+01\n",
      " 9.6659374e+00 1.1199777e+01 1.4959961e+01 1.7883551e+01 4.4954047e+00\n",
      " 2.7638756e+01 4.5674526e+01 5.6632118e+00 3.9772549e+01 2.9067183e+00\n",
      " 2.1288352e+00 1.0493200e+01 9.5713604e-04 2.3805918e-01 2.4986197e+01\n",
      " 1.0150250e+01 1.6615416e+01 2.0356148e+01 6.4832349e+00 4.4740467e+01\n",
      " 1.9642057e+01 1.8522028e+01 4.7635960e+01 3.9360954e+01 1.0883639e+01\n",
      " 1.4986428e+01 5.9608275e-01 1.3978097e+00 3.8560352e+01 1.6704487e+01\n",
      " 4.7609501e+01 3.6767105e+01 7.1874442e+00 2.4691231e+00 5.2734280e+01]\n",
      "After epoch number  5  the cost is  [2.44010985e-01 6.55797043e+01 2.81651560e-02 8.82013798e+00\n",
      " 7.04633147e-02 1.30835466e+01 3.41431084e+01 1.36120481e+01\n",
      " 3.54907721e-01 3.25525713e+00 3.40441895e+00 3.39073682e+00\n",
      " 3.02142739e-01 5.27627602e+01 5.04646264e-03 6.32755876e-01\n",
      " 1.03075325e-01 2.04328175e+01 8.48735504e+01 6.41985953e-01\n",
      " 9.87130356e+00 4.17136879e+01 2.52983112e+01 6.32203668e-02\n",
      " 2.52727509e+01 4.83721199e+01 4.10016022e+01 3.11511917e+01\n",
      " 2.86792603e+01 1.66694680e-03 8.58139229e+00 3.10962939e+00\n",
      " 3.03482399e+01 2.51514797e+01 1.40543938e+01 6.61158752e+00\n",
      " 7.12789059e+00 1.23197680e+01 1.74262428e+01 1.36611685e-01\n",
      " 1.11461744e+01 4.45054579e+00 9.02188683e+00 1.57072112e-01\n",
      " 1.36238174e-03 2.19241028e+01 2.05156975e+01 3.73244357e+00\n",
      " 4.41349106e+01 2.11395457e-01 3.63160521e-01 7.57801114e-03\n",
      " 2.86360722e+01 4.28599179e-01 4.11907196e+01 7.45913535e-02\n",
      " 1.89098835e+01 3.07562619e-01 3.33159518e+00 8.55942443e-02\n",
      " 7.32334805e+00 2.89544582e+00 2.98588409e+01 1.83157563e+00\n",
      " 1.92488956e+00 5.11500359e+00 9.09560680e+00 7.60285378e+00\n",
      " 5.28042078e+00 2.51023788e-02 3.57986999e+00 2.34726830e+01\n",
      " 7.29282475e+00 1.96976070e+01 1.25566924e+00 1.48051244e-04\n",
      " 8.09736443e+00 1.09748650e+00 9.23725253e-04 5.03260493e-01\n",
      " 3.77740240e+00 2.31361351e+01 1.75171700e+01 2.89890021e-01\n",
      " 2.53873158e+01 3.81162435e-01 2.71237907e+01 2.89644647e+00\n",
      " 3.34795570e+01 5.18165302e+00 1.70116699e+00 7.94916010e+00\n",
      " 1.15554594e-03 2.40086346e+01 1.53124704e+01 4.13891373e+01\n",
      " 1.67093658e+01 6.37564611e+00 3.33153531e-02 3.96262016e+01]\n",
      "After epoch number  6  the cost is  [1.55188948e-01 4.37057610e+01 5.19378483e-03 1.70052040e+00\n",
      " 7.47283876e-01 1.19058859e+00 1.79410782e+01 6.97865188e-02\n",
      " 1.24542356e-01 4.63564634e+00 1.69202864e+00 2.05063462e-01\n",
      " 3.48384771e-03 3.69447327e+01 3.12736519e-02 4.66804653e-01\n",
      " 1.43214583e+00 1.06906497e+00 5.95927658e+01 3.03520933e-02\n",
      " 1.33457974e-01 1.54179363e+01 1.20763760e+01 8.70347768e-03\n",
      " 1.92388311e-01 3.38819084e+01 1.40820971e+01 9.76632977e+00\n",
      " 9.71455038e-01 2.64835358e-03 3.52506852e+00 8.95838253e-03\n",
      " 1.36228428e+01 2.71774113e-01 6.09338224e-01 4.43003001e-03\n",
      " 2.30683386e-03 9.21149552e-01 5.48782907e-02 2.17943758e-01\n",
      " 6.36578421e-04 4.93004942e+00 3.82700078e-02 3.82384169e-03\n",
      " 1.77042201e-01 2.71793961e-05 1.94743648e-02 3.51119161e-01\n",
      " 3.54604378e+01 1.13733374e-02 1.29696075e-02 3.64348404e-02\n",
      " 1.97381687e+01 1.30498990e-01 3.17408733e+01 4.53871954e-03\n",
      " 1.47577105e+01 9.41979215e-02 5.29629365e-03 9.71732847e-03\n",
      " 9.42151062e-03 6.37806728e-02 2.21636524e+01 2.05354914e-01\n",
      " 1.34612061e-02 8.84550571e-01 5.41661531e-02 8.86972666e-01\n",
      " 3.44592781e+01 7.20587063e+00 1.28006591e-02 7.49428797e+00\n",
      " 3.26083231e+00 1.11595201e+01 4.91605792e-03 2.09199768e-02\n",
      " 1.84680462e+00 9.26518231e-04 5.93850156e-04 3.17123942e-02\n",
      " 5.25358772e+00 1.73631706e+01 4.34973288e+00 6.29595816e-01\n",
      " 4.36645113e-02 1.58044067e-03 1.94006729e+01 1.45767450e+00\n",
      " 1.23476849e+01 5.23859024e-01 5.36303997e-01 1.42804030e-02\n",
      " 1.49271628e-02 2.53028831e+01 1.72319844e-01 7.64617968e+00\n",
      " 1.03275700e+01 5.16266942e-01 1.48452625e-01 3.24574203e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch number  7  the cost is  [6.99641630e-02 3.66695595e+01 3.83087853e-03 3.48750167e-02\n",
      " 1.15099223e-02 6.26946017e-02 1.54933815e+01 5.56777716e-01\n",
      " 6.50696903e-02 4.49767150e-02 2.19620660e-01 2.95054410e-02\n",
      " 3.37975123e-03 2.63238449e+01 2.77747154e-01 1.31568685e-02\n",
      " 2.82993424e-03 2.13026673e-01 4.13004456e+01 1.44988790e-01\n",
      " 7.43031874e-02 1.14640560e+01 3.07282305e+00 3.49194519e-02\n",
      " 1.01271011e-01 3.51206894e+01 1.94489326e+01 1.16259851e+01\n",
      " 2.37374585e-02 6.09668851e-01 1.86130346e-03 8.39824323e-03\n",
      " 1.25614290e+01 1.00690699e+01 1.16953328e-02 3.62909213e-03\n",
      " 2.09328737e-02 1.41048878e-01 5.67670912e-03 5.20050526e-03\n",
      " 4.49127816e-02 1.86490733e-02 2.55936896e-03 3.87244788e-03\n",
      " 1.48212880e-01 4.92346880e-04 3.26845139e-01 1.90857029e+00\n",
      " 3.59272308e+01 5.10880258e-03 4.10238057e-01 1.51952566e-03\n",
      " 1.80628719e+01 1.82146745e-04 2.23849888e+01 1.83619820e-02\n",
      " 1.16612778e+01 1.80333734e-01 8.62731226e-03 3.23957428e-02\n",
      " 2.39875633e-02 2.74484253e+00 6.10093498e+00 2.94135716e-02\n",
      " 1.25354096e-01 3.64940763e-02 5.59530668e-02 3.16560483e+00\n",
      " 4.13278122e+01 1.55415398e-03 5.70146918e-01 3.23524058e-01\n",
      " 1.20456545e-02 2.06578045e+01 2.41991295e-03 5.78548992e-03\n",
      " 1.29308844e+00 1.23435771e+00 2.04116683e-02 7.84762762e-03\n",
      " 1.66843444e-01 3.39552313e-01 2.83353850e-02 1.45341843e-01\n",
      " 1.21526778e-01 1.31834716e-01 9.38237762e+00 2.31307466e-02\n",
      " 5.38424822e-03 1.29358575e-03 1.50407366e-02 2.15571211e-03\n",
      " 1.18465582e-03 1.69309731e+01 5.03570598e-04 3.94044828e+00\n",
      " 2.10471678e+00 8.60612690e-02 6.35776639e-01 3.64610825e+01]\n",
      "After epoch number  8  the cost is  [1.8759485e-03 1.6292059e+01 1.0996403e-02 6.4968452e-05 1.3593753e-03\n",
      " 3.1167505e-02 6.0649836e-03 1.2363624e-03 5.5947397e-02 7.7845599e-04\n",
      " 2.5232609e-02 2.1223657e-02 6.3988599e-03 2.8285650e+01 1.1085698e-02\n",
      " 2.2617923e-02 1.5597527e-02 5.9041665e-03 2.3501446e+01 6.0453312e-03\n",
      " 9.4070934e-02 2.4510648e+00 4.5186872e+00 1.0983771e-02 1.8575855e-02\n",
      " 2.7223148e+01 3.7250724e+00 7.6892793e-01 3.1789035e-01 5.4837212e-02\n",
      " 1.0675956e-02 4.4240397e-03 8.0814511e-03 8.1826095e-03 6.7240838e-03\n",
      " 2.5779116e-03 4.8166872e-03 5.2768767e-02 6.2147938e-03 4.0616770e-03\n",
      " 2.7735874e-02 2.9790986e-01 7.2833672e-03 4.0693365e-02 1.6569993e-05\n",
      " 6.0143024e-03 2.4669655e-03 2.3948012e-03 3.1961149e+01 1.5975909e-01\n",
      " 2.9370184e-03 3.8576501e-03 5.4546299e+00 6.5548241e-04 2.5488602e+01\n",
      " 2.2152742e-02 2.4404547e+00 2.4816552e-03 8.2382318e-03 2.1433428e-02\n",
      " 4.2919563e-03 2.7416106e-02 2.0173032e-02 1.1695163e-02 2.5991222e-03\n",
      " 6.7312461e-03 4.5614768e-02 3.6888301e+00 2.6946844e+01 3.5927422e-02\n",
      " 1.0700738e-02 9.8199979e-04 7.1632884e-02 7.3157320e+00 4.9248105e-03\n",
      " 2.0575814e-03 2.3421315e-02 6.8641256e-04 8.4091788e-03 2.3568904e-02\n",
      " 7.8464737e-03 2.4070743e-02 3.9214018e-01 2.8338970e-03 1.2940851e-03\n",
      " 6.4399216e-04 2.7014099e-02 2.2669768e-02 1.3093865e-02 1.6272139e-03\n",
      " 7.7248423e-04 2.7904145e-03 2.0632120e-03 1.3532129e+01 3.2935111e-04\n",
      " 5.5094538e+00 2.7391405e-03 5.0307937e-02 1.8284913e-02 1.4352878e+01]\n",
      "After epoch number  9  the cost is  [5.0608213e-03 5.5050888e+00 3.1432835e-04 6.8183313e-04 7.7912374e-04\n",
      " 1.3453876e-02 1.2319811e-02 1.0307927e-02 5.6897130e-02 5.6571595e-04\n",
      " 2.6096573e-02 1.4881986e-02 8.6166486e-03 2.6207142e+01 1.6260011e+00\n",
      " 2.7403612e-02 6.5093039e-04 8.7943804e-03 4.7020206e+00 1.0236471e-02\n",
      " 7.2030229e-03 2.0526361e-01 2.6487189e-04 1.1095281e-01 1.9055732e-02\n",
      " 6.7293458e+00 3.0463089e-03 3.1716775e-03 3.6379823e-04 2.2010418e-02\n",
      " 3.3647761e-02 1.2927430e-02 1.8033062e-03 4.1553741e-03 1.0952438e-02\n",
      " 2.3431676e-03 1.4863975e-02 9.7467480e-03 2.5956626e-03 2.7956147e-02\n",
      " 3.2185218e-03 1.0170031e-03 2.0979901e-03 4.4440357e-03 1.9311810e-05\n",
      " 4.5624597e-04 7.6524061e-03 6.0597970e-03 9.1779118e+00 2.6265448e-03\n",
      " 4.2813085e-03 3.6668577e-03 2.4971104e+00 2.9705025e-03 1.7401876e+01\n",
      " 5.2669670e-02 3.0322771e-03 9.1324141e-03 9.3911663e-03 6.1115954e-02\n",
      " 3.8376104e-03 1.1781762e-02 7.9820640e-03 5.5603618e-03 4.8886449e-03\n",
      " 1.7847519e-02 1.1319256e-02 3.4308514e-01 1.0454062e+01 5.8335550e-03\n",
      " 1.1227896e-02 1.1173118e-03 4.0635802e-03 7.8099525e-01 4.7388722e-04\n",
      " 1.3644414e-02 2.5951918e-02 9.6608623e-04 1.0603174e-03 4.6978861e-02\n",
      " 8.0361897e-03 4.1737638e-02 1.8978868e-02 4.1271653e-03 1.2820093e-03\n",
      " 2.7549118e-03 6.2445835e-03 9.1021750e-03 2.6605124e-02 2.2518199e-03\n",
      " 2.0654476e-03 3.2699993e-03 2.1417136e-03 1.1571465e+01 2.2036259e-03\n",
      " 4.2089932e-03 3.3764034e-03 3.7644131e-03 5.6479261e-03 3.0997242e+01]\n",
      "After epoch number  10  the cost is  [1.1378792e-02 3.3441541e+00 2.8191792e-04 1.6096227e-03 1.5349761e-03\n",
      " 1.6044751e-02 3.5823554e-02 3.3735274e-03 2.4499614e-02 1.0543368e-03\n",
      " 1.8114425e-02 7.2825109e-03 4.3627829e-03 1.6089888e+01 1.8059723e-04\n",
      " 7.9690693e-03 5.7144905e-04 1.9662604e-03 1.3442910e+01 3.7657237e-03\n",
      " 7.9610636e-03 1.6717896e-02 1.0753204e-03 2.3072232e-03 1.6814055e-02\n",
      " 1.4543606e+01 2.0274760e-03 2.8475178e-02 6.3040340e-04 4.6571982e-03\n",
      " 9.7895218e-03 6.7684175e-03 8.4567927e-03 3.1740151e-03 8.7601105e-03\n",
      " 4.4669811e-02 1.3931144e-02 1.4805162e-02 5.1988782e-03 2.9798998e-03\n",
      " 1.5767895e-03 1.8516768e-03 1.2378563e-03 3.7875453e-03 2.0311489e-04\n",
      " 9.7749500e-05 2.0343768e-03 8.7005564e-04 6.8310852e+00 3.9995206e-03\n",
      " 4.8296436e-04 3.7903837e-03 1.6199846e-02 3.2817651e-02 1.3173802e+01\n",
      " 2.4089825e-03 3.1093801e-03 4.6459422e-03 9.4667375e-03 7.1544209e-03\n",
      " 3.7025430e-03 9.2912260e-03 2.4822809e-02 7.6812329e-03 3.3888346e-04\n",
      " 3.2588311e-02 2.6467431e-02 1.4842701e-02 7.3636560e+00 2.9388438e-03\n",
      " 9.0608690e-03 6.1531644e-04 8.4493523e-03 1.5186906e-04 1.2743175e-04\n",
      " 1.1210672e-03 5.1326123e-03 6.8234652e-02 6.1875749e-03 2.7653011e-02\n",
      " 3.7566156e-03 3.5641346e-02 7.0394054e-03 3.7831445e-03 2.0887761e-03\n",
      " 1.2516490e-03 7.5496780e-04 2.0080425e-03 1.1799208e-02 6.0299910e-03\n",
      " 1.9333190e-03 7.4520172e-03 2.0555211e-03 4.4469748e+00 2.4789731e-01\n",
      " 2.8652351e-03 6.0512852e-03 7.6550441e-03 3.9023850e-03 9.2972383e+00]\n",
      "Accuracy is  0.0\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network( X ):\n",
    "    predictions = neural_network_model(X)\n",
    "    cost = tf.nn.softmax_cross_entropy_with_logits( logits=predictions,labels=y )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run( tf.global_variables_initializer() )\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            start = 0\n",
    "            end = start + batch_size\n",
    "            epoch_cost = 0\n",
    "            \n",
    "            while start < len(train_X):\n",
    "                epoch_x = train_X[start:end]\n",
    "                epoch_y = train_y[start:end]\n",
    "                \n",
    "                c,_ = sess.run( [cost,optimizer],feed_dict = {X:epoch_x,y:epoch_y} )\n",
    "                epoch_cost += c\n",
    "                \n",
    "                start = end\n",
    "                end = min( len(train_X), start + batch_size)\n",
    "            print (\"After epoch number \", epoch+1 , \" the cost is \", epoch_cost)\n",
    "            \n",
    "        correct = tf.equal( tf.argmax(predictions), tf.argmax(y) )\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
    "        print (\"Accuracy is \", accuracy.eval(feed_dict = {X:test_X,y:test_y})  )\n",
    "        \n",
    "train_neural_network(X)\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
